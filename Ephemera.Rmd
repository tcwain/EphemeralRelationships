---
title: "Ephemeral Relationships Project"
author: "Tom Wainwright, NOAA Northwest Fisheries Science Center"
date: 'Updated: `r Sys.Date()`'
output:
  html_document:
    fig_caption: yes
    highlight: tango
    self_contained: no
    theme: journal
  pdf_document:
    fig_caption: yes
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height=4, fig.width=6, warning=TRUE)
## load(".RData") #not done automatically in interactive knitr session.
```

## Introduction

**NOTE:** This is an R Markdown document. Markdown is a simple formatting syntax for authoring 
HTML, PDF, and MS Word documents. For more details on using R Markdown see 
<http://rmarkdown.rstudio.com>.

Besides R base and recommended packages, this requires R packages `akima` and `RColorBrewer`.

## Data

Update the environmental data sets, if desired (edit value of "UPDATE.DATA"). This only needs to
be done if the file "AnalysisData.csv" is out-of-date.

**Warning**: Updating data is error-prone, as URLs and file formats change frequently. 
Check output for warnings and errors and correct the UpdateData and ProcessData scripts 
as needed before finalizing the analysis.

**Note**: Biological data (Coho populations and "stoplight" datasets) and Loggerwell Spring 
Transition (SpringTrans.csv)  must be updated by hand, and files put in the correct locations! 
Charleston water temperature historical data (WT.OIMB.*) files should never change, but must 
be present.

```{r UpdateData}
UPDATE.DATA <- TRUE
if (UPDATE.DATA) { 
  options(warn=1)  # Print all messages
  
  olddir <- setwd('R-scripts') # Move into scripts directory
  print(getwd())
  list.files()
  
  d.dir <- file.path('..', 'Data') # Data directory
  dd.dir <- file.path(d.dir, 'Download') # Downloaded data directory
  hand.dir <- file.path(d.dir, 'HandEdit') # Hand-edit data directory

  cat('\nUpdating source datasets.\n')
  source('UpdateData.R', echo=F)
  
  cat('\nComputing derived datasets.\n')
  source('ProcessData.R', echo=F)
  
  cat('\nWriting final analysis file.\n')
  source('ConsolidateData.R', echo=F)
  
  setwd(olddir) # Restore working directory
}
```

Read in the data. Monthly environmental series are restructured to 4 seasons (Win [JFM], Spr [AMJ], Sum [JAS], & Aut [OND]). Annual environmental series are left as is. OCN coho recruits and parental spanwer abundance are lagged to match the environment during ocean entry, so no need to lag within the analysis; do a plot so we can double check this alignment against published data sets.

```{r ReadData}
# Environmental data:
analdata <- read.csv(file.path("Data", "AnalysisData.csv"), header=TRUE)
# shorten NPGO abbrev. to 3 letters:
names(analdata) <- sub('NPGO_', 'NPG_', names(analdata))
analdata <- analdata[!is.na(analdata$OCN.RCR), ] # Use only years with OCN recruit data.
# Plot to check data alignment
with(analdata, {
  plot(YEAR, scale(OCN.RCR), type='l', ylim=c(-3,3), tck=1);
  lines(YEAR, scale(OCN.SPN), col=2);
  lines(YEAR, scale(PDO_AMJ), col=3)
  })
```
Yes, the data line up properly. The 2011 and 2014 peaks in recruits and spawners are plotted in 
ocean entry years 2010 and 2013 for recruits returning 1 year later, and in years 2013 and 2016 
for spawners that entered ocean at age 2.

Select predictors. Change sign of NPGO, NPI & UWI so they correlate positively with other varialbles.

```{r SelectPredictors}
predictors <- analdata[ , -(1:3)]  # Whole data set except Year and OCN coho data
## reverse sign on some predictors to have consistent +/- regr. slopes
predictors[ , grep('NPG_', colnames(predictors))] <- 
  - predictors[ , grep('NPG_', colnames(predictors))]
predictors[ , grep('UWI_', colnames(predictors))] <- 
  - predictors[ , grep('UWI_', colnames(predictors))]
plot.names <- colnames(predictors)   # FUTURE: Pretty names
## Testing - subset of predictors 
## predictors <- predictors[ , 1:12]
```

## Analysis

### Some function definitions

First, a function to compute the leave-one-out cv skill score for a bivariate linear model, defined as:
$$
1 - \frac { (n-1) \sum (\hat Y_{(i)} - Y_i)^2 } { (n-2) \sum (\bar Y - Y_i)^2}
$$
where the sums are over the $n$ data points and $\hat Y_{(i)}$ is the predicted value of $Y_i$ from the model with point $i$ left out.

Crude, but effective. Should be able to do this more efficiently for a linear model.

```{r LOOFunc}
loo.skill <- compiler::cmpfun(function(df) {
  # Leave-one-out cross-validation, scaled to skill score, for a 
  #   bivariate linear regr.
  # df is a dataframe with two columns: X & Y
  ## check for sufficient data (non-NAs for X & Y in at least 5 cases)
  if (sum(!is.na(df$X+df$Y)) > 4) { # OK
    ybar <- mean(df$Y)
    n <- nrow(df)
    loo.pred <- function(i) predict(lm(Y ~ X, data=df[-i,]), newdata=df[i,])
    .pred <- sapply(1:n, loo.pred)
    loo.cvs <- 1 - (n-1) / (n-2) * sum((.pred - df$Y)^2) / sum((ybar - df$Y)^2)
    return(list(CVS=loo.cvs, X=df$X, Y=df$Y, Pred=.pred))
  } else { # Insuffient data
    return(list(CVS=NA, X=df$X, Y=df$Y, Pred=NULL))
  } # if (sum...)
}) # loo.skill
```

```{r TestLOO, echo=FALSE, eval=FALSE}
testN <- 10
# Perfect relationship:
X <- Y <- rnorm(testN)
tdf <- data.frame(X=X,Y=Y)
res <-loo.skill(tdf)  
print(res$CVS) # should be 1.0
# No relationship:
tdf$Y <- rnorm(testN)
res <-loo.skill(tdf)  
print(res$CVS) # should be near 0.0 for large testN
plot(res$X, res$Y, xlab="X", ylab="Y")
points(res$X, res$Pred, pch='+', col='blue')
# 50% relationship, multiple trials:
res <- c()
for (k in 1:25) {
  tdf$Y <- tdf$X + rnorm(testN) # independent error 
  res <- c(res, loo.skill(tdf)$CVS) # should be near 0.5 for large testN
  }
summary(res)
rm(X, Y, tdf, res)
```

Next, a function to compute the model fit at all scales. For a multivariate data time series of length N, applies a goodness-of-fit function to all overlapping subseries of length n, where n runs from a lower limit k (10 by default) to the entire series length. Results are stored in an N x (N-k+1) matrix suitable for plotting via ``image``.

```{r tsapplyFunc}
tsapply <- compiler::cmpfun(function(dataser, goffnc, minlen=10) {
  # dataser is a multivariate time series, in the form of a data.frame
  # goffnc is a function that fits a model to a data frame, and returns 
  #   a single numeric goodness-of-fit measure.
  # minlen is the minumum data subset length
  N <- nrow(dataser)
  res <- data.frame(time=NULL, scale=NULL, gof=NULL)
  for (n in minlen:N) {
    for (j in 1:(N-n+1)) {
      .sub <- j + 0:(n-1)
      if (length(.sub) != n) stop("length(.sub) = ", length(.sub), ", n = ", n)
      .dat <- dataser[.sub, ]
      res <- rbind(res, data.frame(time=max(.sub), scale=n, gof=goffnc(.dat)))
    } # for(t)
  } # for(n)
  return(res)
}) # tsapply
```

We want a graphic that shows both the strength of the relationship (skill) and its direction (sign of the slope), so define a "signed skill" metric by truncating the skill measure to be non-negative (negative skill means really bad relationship anyway), then multiplying by the sign of the regression slope. Also make a function to plot the resulting metric as a function of time and scale.

In the plot, color indicates sign (blue positive, red negative) and circle size indicates skill. Contours are added at skill levels 0.25 (thin dotted), 0.5 (dashed) and 0.75 (solid).

```{r slopeSkill, fig.height=6, fig.width=6}
slpSkill <- function(X, Y) {
  #return LOO skill and regression slope in one data frame
  loo.res <- tsapply(data.frame(X=X,Y=Y), function(df) {loo.skill(df)$CVS})
  # get regression slopes for color-coding graph
  ### TODO: fix gof code for missing data cases ###
  slp.res <- tsapply(data.frame(X=X,Y=Y), function(df) {
    if (sum(!is.na(df$X+df$Y)) > 4) { # OK
      return(coefficients(lm(Y ~ X, data=df))['X'])
    } else {
      return(NA)
    } # if(sum...)
  } # function(df)
  ) # tsapply()
  loo.res$slp <- slp.res$gof
  return(loo.res)
} # slpSkill()

skillPlot <- function(ss, title="", dot.scale=0.5, ...) {
  # Plot signed skill on scale v time axis
  ss <- ss[!is.na(ss$gof), ]      # remove na's for interp()
  ss$gof <- pmax(0,ss$gof) # get rid of negative values (very bad fit)
  plot(ss$time, ss$scale, type='n', asp=1.0, xaxs='r', 
       xlab='Interval Endpoint', ylab='Interval Length', ...)
  pl.lims <- par('usr') # get X & Y plot limits (Xmin,Xmax,Ymin,Ymax)
  xrange <- pl.lims[2] - pl.lims[1]
  yrange <- pl.lims[4] - pl.lims[3]
#  text(min(ss$time), max(ss$scale), title, cex=1.25, adj=c(0, 1), col='black')
  text(pl.lims[1]+0.05*xrange, pl.lims[4]-0.05*yrange, title, cex=1.2, adj=c(0, 1), col='black')
#  text(min(ss$time), max(ss$scale) - 0.1 * diff(range(ss$scale)),
  text(pl.lims[1]+0.05*xrange, pl.lims[4]-0.1*yrange,
       paste0("Long-term Skill: ", round(ss$gof[ss$scale==max(ss$scale)],2)), 
       cex=1, adj=c(0,1), col='black')
#  text(min(ss$time), max(ss$scale) - 0.17 * diff(range(ss$scale)),
  text(pl.lims[1]+0.05*xrange, pl.lims[4]-0.15*yrange,
       paste0("Maximum Skill: ", round(max(ss$gof),2)), 
       cex=1, adj=c(0,1), col='black')
  symbols(ss$time, ss$scale,
            circles=dot.scale * sqrt(ss$gof), inches=FALSE, fg='grey80',
            bg=ifelse(ss$slp<0,"#D34364","#4364D3"), add=T)
  zz <- with(ss, akima::interp(time, scale, gof, xo=sort(unique(time)), 
                               yo=sort(unique(scale))))
  contour(zz, levels=c(0.25, 0.5, 0.75), 
          lty=c(3, 2, 1), lwd=c(1, 2, 2), col='black', 
          drawlabels=FALSE, add=TRUE)
} # skillPlot()
X <- rnorm(30)
Y <- X + rnorm(30)
tst <- slpSkill(X, Y)
skillPlot(tst, title='Test')
rm(X, Y, tst)
```

### Test with AR(1) random series

Generate 12 pairs of random AR(1) series, plot patterns.

```{r ARtest, fig.height=6, fig.width=9}
op <- par(mfrow=c(3,4), mar=c(2,2,1,1), mgp=c(3,0.5,0), fg='grey', tck=1)
for (i in 1:12) {
  X <- arima.sim(n=30, model=list(ar=0.7))
  Y <- arima.sim(n=30, model=list(ar=0.7))
  tst <- slpSkill(X, Y)
  skillPlot(tst, title=paste0('Tst_AR1_',i))
  rm(X, Y, tst)
}
par(op)
```

## Real Data

Compute skill and sign of relationship for all years and scales across selected predictors. (Only run if resulting output doesn't already exist, so to refresh results, delete ``ss.out`` before running.)

```{r AnalLoop}
if (!exists("ss.out")) {
  library(parallel)
  mc <- max(1, floor(detectCores()/2)) # use ca. half of available processors
  preds <- as.list(names(predictors))
  names(preds) <- names(predictors)   # so result is a named list
##  ss.out <- mclapply(preds, 
  ss.out <- lapply(preds, 
                     function (k) {
                       x <- log(analdata$OCN.RCR)
                       slpSkill(X=x, Y=predictors[ , k])
                     }
  )
##                     , mc.cores=mc)
  rm(mc, preds)
} # if (!exists(ss.out))
```

Summarize results and plot. Plots are done only for those with skill above a threshold. The X-axis is the year of ocean entry (return year - 1).

```{r Results, fig.height=6, fig.width=6}
skillthresh <- 0.00
op <- par(mfrow=c(2,2), mar=c(2,2,1,1), mgp=c(3,0.5,0), fg='grey', tck=1)
for (series in names(ss.out)) {
  cat('Predictor: ', series, '\n')
  ss <- ss.out[[series]]
  ss$time <- analdata$YEAR[ss$time]
  cat('\t Range of Skill: ', round(range(ss$gof, na.rm=TRUE),2), '\n')
##  if (max(ss$gof, na.rm=TRUE) > skillthresh) { 
    skillPlot(ss, title=series, dot.scale=1.0, xlim=c(1975,2015), ylim=c(10,50)) 
##  } # if
} # for(series)
par(op)
rm(skillthresh, op)
```

Look at some clustering patterns in the raw data and in the patterns of skill.

```{r Clustering1, fig.height=9, fig.width=9}
# raw Data
library(MASS)
palette(c("gray40",RColorBrewer::brewer.pal(7,"Dark2")))
dat <- as.matrix(cbind(RECR=log(analdata$OCN.RCR), predictors))
dat <- dat[ !apply(dat, 1, function(x) any(is.na(x))), ]
image(cor(dat), col=RColorBrewer::brewer.pal(11,'RdBu'))
d1 <- as.dist(1 - abs(cor(dat)))             # correlation-based distances
h1 <- hclust(d1, method="single")
clcode <- cutree(h1, 8)
plot(h1, cex=0.5)
data.mds <- cmdscale(d1, k=2)
eqscplot(data.mds, type='n')
text(data.mds, labels=rownames(data.mds), cex=0.5, col=clcode)
rm(dat, d1, h1, clcode, data.mds)
```

```{r Clustering2, fig.height=9, fig.width=9, eval=FALSE}
# cluster based on similarity of skill patterns
dat <- as.data.frame(lapply(ss.out, "[[", "gof"))
dat <- dat[, -grep("YEAR",colnames(dat))]
dat[dat<0] <- 0
d2 <- dist(t(dat))
h2 <- hclust(d2, method="single")
plot(h2, cex=0.5)
clcode <- cutree(h2, 6)
data.mds <- cmdscale(d2, k=2)
eqscplot(data.mds, type='n')
text(data.mds, labels=rownames(data.mds), cex=0.5, col=clcode)
rm(dat, d2, h2, clcode, data.mds)
```

```{r cleanup, echo=FALSE}
ls() # see what's being saved
save.image() # save data for re-use
```
